{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# ğŸ“ˆ Sales Forecasting - Time Series Analysis\n",
    "\n",
    "**Project**: Sales Forecasting & Business Intelligence Dashboard  \n",
    "**Author**: [Your Name]  \n",
    "**Date**: September 2024\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Notebook Overview\n",
    "\n",
    "Welcome to my comprehensive sales forecasting analysis! In this notebook, I'll build and compare multiple forecasting models to predict future sales trends. \n",
    "\n",
    "### What we'll accomplish:\n",
    "- ğŸ“Š **Exploratory Data Analysis** - Understanding sales patterns and trends\n",
    "- ğŸ” **Time Series Decomposition** - Identifying trend, seasonality, and noise\n",
    "- ğŸ¤– **Multiple Forecasting Models** - ARIMA, Prophet, Random Forest, Linear Regression\n",
    "- ğŸ“ˆ **Model Evaluation** - Comparing accuracy and business applicability\n",
    "- ğŸ’¡ **Business Insights** - Actionable recommendations for stakeholders\n",
    "\n",
    "This analysis will demonstrate my ability to extract business value from time series data and build production-ready forecasting systems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Time series and forecasting\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prophet for advanced time series forecasting\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Prophet not available. Install with: pip install prophet\")\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ“ˆ Sales Forecasting Analysis - Ready to Go!\")\n",
    "print(f\"ğŸ• Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ”® Prophet available: {PROPHET_AVAILABLE}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## ğŸ“Š Data Generation & Loading\n",
    "\n",
    "For this demonstration, I'll create realistic sales data that mimics real-world patterns including trends, seasonality, and business cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "source": [
    "# Generate realistic sales data\n",
    "def generate_sales_data():\n",
    "    \"\"\"Generate realistic sales data with trends, seasonality, and noise\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ Generating realistic sales dataset...\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create date range (3 years of daily data)\n",
    "    start_date = datetime(2021, 1, 1)\n",
    "    end_date = datetime(2024, 9, 30)\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    print(f\"ğŸ“… Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"ğŸ“Š Total days: {len(dates)}\")\n",
    "    \n",
    "    # Base sales level\n",
    "    base_sales = 3000\n",
    "    \n",
    "    # Long-term trend (growth over time)\n",
    "    trend = np.linspace(0, 500, len(dates))\n",
    "    \n",
    "    # Annual seasonality (peak in Q4, low in Q1)\n",
    "    annual_seasonality = 400 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25 + np.pi/2)\n",
    "    \n",
    "    # Weekly seasonality (higher on weekends)\n",
    "    weekly_seasonality = 200 * np.sin(2 * np.pi * np.arange(len(dates)) / 7 + np.pi/2)\n",
    "    \n",
    "    # Holiday effects (Black Friday, Christmas, etc.)\n",
    "    holiday_boost = np.zeros(len(dates))\n",
    "    \n",
    "    for i, date in enumerate(dates):\n",
    "        # Black Friday (4th Thursday of November)\n",
    "        if date.month == 11 and date.weekday() == 4:  # Friday\n",
    "            thursday = date - timedelta(days=1)\n",
    "            if 22 <= thursday.day <= 28:  # 4th Thursday\n",
    "                holiday_boost[i] = 2000\n",
    "        \n",
    "        # Christmas season (December)\n",
    "        elif date.month == 12:\n",
    "            holiday_boost[i] = 800 * (1 + np.sin(np.pi * date.day / 31))\n",
    "        \n",
    "        # Back to school (August)\n",
    "        elif date.month == 8:\n",
    "            holiday_boost[i] = 300\n",
    "        \n",
    "        # Summer boost (June-July)\n",
    "        elif date.month in [6, 7]:\n",
    "            holiday_boost[i] = 200\n",
    "    \n",
    "    # Random noise (business volatility)\n",
    "    noise = np.random.normal(0, 300, len(dates))\n",
    "    \n",
    "    # Combine all components\n",
    "    sales = base_sales + trend + annual_seasonality + weekly_seasonality + holiday_boost + noise\n",
    "    \n",
    "    # Ensure no negative sales\n",
    "    sales = np.maximum(sales, 100)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    sales_data = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'sales': sales.round(2),\n",
    "        'year': dates.year,\n",
    "        'month': dates.month,\n",
    "        'day': dates.day,\n",
    "        'weekday': dates.weekday,\n",
    "        'is_weekend': (dates.weekday >= 5).astype(int),\n",
    "        'is_holiday_season': ((dates.month == 12) | (dates.month == 11)).astype(int)\n",
    "    })\n",
    "    \n",
    "    # Add additional business features\n",
    "    sales_data['sales_lag1'] = sales_data['sales'].shift(1)\n",
    "    sales_data['sales_lag7'] = sales_data['sales'].shift(7)\n",
    "    sales_data['sales_ma7'] = sales_data['sales'].rolling(window=7).mean()\n",
    "    sales_data['sales_ma30'] = sales_data['sales'].rolling(window=30).mean()\n",
    "    \n",
    "    print(f\"ğŸ’° Average daily sales: ${sales_data['sales'].mean():,.0f}\")\n",
    "    print(f\"ğŸ“Š Sales range: ${sales_data['sales'].min():,.0f} - ${sales_data['sales'].max():,.0f}\")\n",
    "    print(f\"ğŸ“ˆ Total revenue: ${sales_data['sales'].sum():,.0f}\")\n",
    "    \n",
    "    return sales_data\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_sales_data()\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nğŸ“‹ Dataset Overview:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Columns: {list(df.columns)}\")\n",
    "print(f\"   Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"   Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nğŸ“Š Sample Data:\")\n",
    "display(df.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## ğŸ“Š Exploratory Data Analysis\n",
    "\n",
    "Let's explore our sales data to understand patterns, trends, and seasonality that will inform our forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "source": [
    "# Set date as index for time series analysis\n",
    "df_ts = df.set_index('date').copy()\n",
    "\n",
    "# Basic time series visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('ğŸ“ˆ Sales Data Exploratory Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Overall time series\n",
    "axes[0,0].plot(df_ts.index, df_ts['sales'], color='navy', alpha=0.7, linewidth=1)\n",
    "axes[0,0].set_title('Daily Sales Over Time')\n",
    "axes[0,0].set_ylabel('Sales ($)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(range(len(df_ts)), df_ts['sales'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0,0].plot(df_ts.index, p(range(len(df_ts))), \"r--\", alpha=0.8, linewidth=2, label='Trend')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. Sales distribution\n",
    "axes[0,1].hist(df_ts['sales'], bins=50, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "axes[0,1].axvline(df_ts['sales'].mean(), color='red', linestyle='--', \n",
    "                  label=f'Mean: ${df_ts[\"sales\"].mean():.0f}')\n",
    "axes[0,1].axvline(df_ts['sales'].median(), color='green', linestyle='--',\n",
    "                  label=f'Median: ${df_ts[\"sales\"].median():.0f}')\n",
    "axes[0,1].set_title('Sales Distribution')\n",
    "axes[0,1].set_xlabel('Sales ($)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Monthly aggregation\n",
    "monthly_sales = df_ts.groupby([df_ts.index.year, df_ts.index.month])['sales'].sum()\n",
    "monthly_dates = pd.to_datetime([f'{year}-{month:02d}-01' for year, month in monthly_sales.index])\n",
    "\n",
    "axes[1,0].plot(monthly_dates, monthly_sales.values, marker='o', linewidth=2, markersize=4, color='green')\n",
    "axes[1,0].set_title('Monthly Sales Totals')\n",
    "axes[1,0].set_ylabel('Monthly Sales ($)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Weekly seasonality\n",
    "weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "weekday_sales = df_ts.groupby('weekday')['sales'].mean()\n",
    "\n",
    "bars = axes[1,1].bar(range(7), weekday_sales.values, color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[1,1].set_xticks(range(7))\n",
    "axes[1,1].set_xticklabels(weekday_names)\n",
    "axes[1,1].set_title('Average Sales by Day of Week')\n",
    "axes[1,1].set_ylabel('Average Sales ($)')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "                   f'${height:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(\"ğŸ” Key Insights from EDA:\")\n",
    "print(f\"   ğŸ’° Average daily sales: ${df_ts['sales'].mean():,.0f}\")\n",
    "print(f\"   ğŸ“Š Sales volatility (std): ${df_ts['sales'].std():,.0f}\")\n",
    "print(f\"   ğŸ“ˆ Growth trend: {(df_ts['sales'].iloc[-365:].mean() - df_ts['sales'].iloc[:365].mean()):.0f} $/year\")\n",
    "print(f\"   ğŸ“… Strongest day: {weekday_names[weekday_sales.idxmax()]} (${weekday_sales.max():.0f})\")\n",
    "print(f\"   ğŸ“… Weakest day: {weekday_names[weekday_sales.idxmin()]} (${weekday_sales.min():.0f})\")\n",
    "print(f\"   ğŸ¯ Weekend boost: {((weekday_sales[5:].mean() - weekday_sales[:5].mean()) / weekday_sales[:5].mean() * 100):.1f}%\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## ğŸ” Time Series Decomposition\n",
    "\n",
    "Let's decompose our time series into its fundamental components: trend, seasonality, and residuals. This analysis will help us understand the underlying patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "source": [
    "# Perform seasonal decomposition\n",
    "print(\"ğŸ”„ Performing seasonal decomposition...\")\n",
    "\n",
    "# Use additive decomposition with weekly seasonality (period=7)\n",
    "decomposition = seasonal_decompose(df_ts['sales'], model='additive', period=7)\n",
    "\n",
    "# Create decomposition plot\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 12))\n",
    "fig.suptitle('ğŸ“Š Time Series Decomposition Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Original time series\n",
    "decomposition.observed.plot(ax=axes[0], color='navy', title='Original Sales Data')\n",
    "axes[0].set_ylabel('Sales ($)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Trend component\n",
    "decomposition.trend.plot(ax=axes[1], color='red', title='Trend Component')\n",
    "axes[1].set_ylabel('Trend ($)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Seasonal component\n",
    "decomposition.seasonal.plot(ax=axes[2], color='green', title='Seasonal Component (Weekly)')\n",
    "axes[2].set_ylabel('Seasonal ($)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual component\n",
    "decomposition.resid.plot(ax=axes[3], color='orange', title='Residual Component')\n",
    "axes[3].set_ylabel('Residual ($)')\n",
    "axes[3].set_xlabel('Date')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze decomposition components\n",
    "print(\"\\nğŸ“Š Decomposition Analysis:\")\n",
    "print(f\"   ğŸ“ˆ Trend strength: {1 - (decomposition.resid.var() / decomposition.observed.var()):.3f}\")\n",
    "print(f\"   ğŸ”„ Seasonal strength: {1 - (decomposition.resid.var() / (decomposition.observed - decomposition.trend).var()):.3f}\")\n",
    "print(f\"   ğŸ“Š Trend range: ${decomposition.trend.min():.0f} - ${decomposition.trend.max():.0f}\")\n",
    "print(f\"   ğŸ”„ Seasonal range: ${decomposition.seasonal.min():.0f} - ${decomposition.seasonal.max():.0f}\")\n",
    "print(f\"   ğŸ“Š Residual std: ${decomposition.resid.std():.0f}\")\n",
    "\n",
    "# Monthly seasonality analysis\n",
    "print(\"\\nğŸ“… Monthly Seasonal Patterns:\")\n",
    "monthly_avg = df_ts.groupby(df_ts.index.month)['sales'].mean()\n",
    "overall_avg = df_ts['sales'].mean()\n",
    "\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "for month, avg_sales in monthly_avg.items():\n",
    "    deviation = ((avg_sales - overall_avg) / overall_avg) * 100\n",
    "    print(f\"   {month_names[month-1]}: ${avg_sales:.0f} ({deviation:+.1f}% vs average)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## ğŸ§ª Stationarity Testing\n",
    "\n",
    "Before building ARIMA models, we need to test if our time series is stationary. Non-stationary data needs to be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "source": [
    "# Test for stationarity using Augmented Dickey-Fuller test\n",
    "def test_stationarity(timeseries, title):\n",
    "    \"\"\"Perform ADF test and visualize stationarity\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Stationarity Test: {title}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Perform ADF test\n",
    "    adf_result = adfuller(timeseries.dropna())\n",
    "    \n",
    "    print(f\"ADF Statistic: {adf_result[0]:.6f}\")\n",
    "    print(f\"p-value: {adf_result[1]:.6f}\")\n",
    "    print(\"Critical Values:\")\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f\"\\t{key}: {value:.3f}\")\n",
    "    \n",
    "    # Interpret results\n",
    "    if adf_result[1] <= 0.05:\n",
    "        print(\"âœ… Series is STATIONARY (reject null hypothesis)\")\n",
    "        stationary = True\n",
    "    else:\n",
    "        print(\"âŒ Series is NON-STATIONARY (fail to reject null hypothesis)\")\n",
    "        stationary = False\n",
    "    \n",
    "    return stationary, adf_result\n",
    "\n",
    "# Test original series\n",
    "is_stationary, adf_original = test_stationarity(df_ts['sales'], \"Original Sales Data\")\n",
    "\n",
    "# If not stationary, try differencing\n",
    "if not is_stationary:\n",
    "    print(\"\\nğŸ”„ Applying differencing to achieve stationarity...\")\n",
    "    \n",
    "    # First difference\n",
    "    df_ts['sales_diff1'] = df_ts['sales'].diff()\n",
    "    is_stationary_diff1, adf_diff1 = test_stationarity(df_ts['sales_diff1'], \"First Differenced Data\")\n",
    "    \n",
    "    # Second difference if needed\n",
    "    if not is_stationary_diff1:\n",
    "        df_ts['sales_diff2'] = df_ts['sales_diff1'].diff()\n",
    "        is_stationary_diff2, adf_diff2 = test_stationarity(df_ts['sales_diff2'], \"Second Differenced Data\")\n",
    "\n",
    "# Visualize stationarity\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
    "fig.suptitle('ğŸ“Š Stationarity Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Original series\n",
    "axes[0].plot(df_ts.index, df_ts['sales'], color='navy', alpha=0.7)\n",
    "axes[0].set_title(f'Original Series (ADF p-value: {adf_original[1]:.4f})')\n",
    "axes[0].set_ylabel('Sales ($)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# First difference\n",
    "if 'sales_diff1' in df_ts.columns:\n",
    "    axes[1].plot(df_ts.index, df_ts['sales_diff1'], color='red', alpha=0.7)\n",
    "    axes[1].set_title(f'First Difference (ADF p-value: {adf_diff1[1]:.4f})')\n",
    "    axes[1].set_ylabel('First Diff ($)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Rolling statistics for first difference\n",
    "    rolling_mean = df_ts['sales_diff1'].rolling(window=30).mean()\n",
    "    rolling_std = df_ts['sales_diff1'].rolling(window=30).std()\n",
    "    \n",
    "    axes[2].plot(df_ts.index, rolling_mean, color='green', label='30-day Rolling Mean')\n",
    "    axes[2].plot(df_ts.index, rolling_std, color='orange', label='30-day Rolling Std')\n",
    "    axes[2].set_title('Rolling Statistics (First Difference)')\n",
    "    axes[2].set_xlabel('Date')\n",
    "    axes[2].set_ylabel('Value')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine optimal differencing order\n",
    "if is_stationary:\n",
    "    d_order = 0\n",
    "    working_series = df_ts['sales']\n",
    "    print(\"\\nâœ… Using original series (d=0)\")\n",
    "elif 'sales_diff1' in df_ts.columns and is_stationary_diff1:\n",
    "    d_order = 1\n",
    "    working_series = df_ts['sales_diff1']\n",
    "    print(\"\\nâœ… Using first difference (d=1)\")\n",
    "elif 'sales_diff2' in df_ts.columns and is_stationary_diff2:\n",
    "    d_order = 2\n",
    "    working_series = df_ts['sales_diff2']\n",
    "    print(\"\\nâœ… Using second difference (d=2)\")\n",
    "else:\n",
    "    d_order = 1  # Default fallback\n",
    "    working_series = df_ts['sales_diff1'] if 'sales_diff1' in df_ts.columns else df_ts['sales']\n",
    "    print(\"\\nâš ï¸ Using default differencing (d=1)\")\n",
    "\n",
    "print(f\"ğŸ“Š Optimal differencing order for ARIMA: d={d_order}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## ğŸ¤– Model Building & Training\n",
    "\n",
    "Now let's build and train multiple forecasting models to find the best approach for our sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "source": [
    "# Prepare data for modeling\n",
    "print(\"ğŸ”„ Preparing data for modeling...\")\n",
    "\n",
    "# Split data into train/test sets (80/20 split)\n",
    "split_date = df_ts.index[int(len(df_ts) * 0.8)]\n",
    "train_data = df_ts[df_ts.index <= split_date].copy()\n",
    "test_data = df_ts[df_ts.index > split_date].copy()\n",
    "\n",
    "print(f\"ğŸ“Š Training data: {len(train_data)} days ({train_data.index.min()} to {train_data.index.max()})\")\n",
    "print(f\"ğŸ“Š Testing data: {len(test_data)} days ({test_data.index.min()} to {test_data.index.max()})\")\n",
    "\n",
    "# Initialize model results dictionary\n",
    "model_results = {}\n",
    "\n",
    "# Define evaluation metrics\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    \n",
    "    # Handle any NaN values\n",
    "    valid_mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "    y_true_clean = y_true[valid_mask]\n",
    "    y_pred_clean = y_pred[valid_mask]\n",
    "    \n",
    "    if len(y_true_clean) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
    "    mape = np.mean(np.abs((y_true_clean - y_pred_clean) / y_true_clean)) * 100\n",
    "    r2 = r2_score(y_true_clean, y_pred_clean)\n",
    "    \n",
    "    # Direction accuracy (did we predict the trend correctly?)\n",
    "    if len(y_true_clean) > 1:\n",
    "        true_direction = np.diff(y_true_clean) > 0\n",
    "        pred_direction = np.diff(y_pred_clean) > 0\n",
    "        direction_accuracy = np.mean(true_direction == pred_direction) * 100\n",
    "    else:\n",
    "        direction_accuracy = 0\n",
    "    \n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'mape': mape,\n",
    "        'r2': r2,\n",
    "        'direction_accuracy': direction_accuracy\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"\\nâœ… Data preparation completed!\")\n",
    "print(f\"   Training set average: ${train_data['sales'].mean():.0f}\")\n",
    "print(f\"   Test set average: ${test_data['sales'].mean():.0f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "source": [
    "# Model 1: ARIMA\n",
    "print(\"ğŸ”„ Training ARIMA Model...\")\n",
    "\n",
    "try:\n",
    "    # Use automated ARIMA parameter selection or set manually\n",
    "    # For this demo, we'll use simple parameters based on our stationarity analysis\n",
    "    arima_order = (1, d_order, 1)  # (p, d, q)\n",
    "    \n",
    "    print(f\"   ARIMA order: {arima_order}\")\n",
    "    \n",
    "    # Fit ARIMA model\n",
    "    arima_model = ARIMA(train_data['sales'], order=arima_order)\n",
    "    arima_fitted = arima_model.fit()\n",
    "    \n",
    "    # Make predictions\n",
    "    arima_forecast = arima_fitted.forecast(steps=len(test_data))\n",
    "    arima_conf_int = arima_fitted.get_forecast(steps=len(test_data)).conf_int()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    arima_metrics = calculate_metrics(test_data['sales'].values, arima_forecast, 'ARIMA')\n",
    "    model_results['ARIMA'] = {\n",
    "        'predictions': arima_forecast,\n",
    "        'conf_int': arima_conf_int,\n",
    "        'metrics': arima_metrics,\n",
    "        'model': arima_fitted\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… ARIMA MAPE: {arima_metrics['mape']:.2f}%\")\n",
    "    print(f\"   âœ… ARIMA RÂ²: {arima_metrics['r2']:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ ARIMA model failed: {e}\")\n",
    "    model_results['ARIMA'] = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "source": [
    "# Model 2: Prophet (if available)\n",
    "if PROPHET_AVAILABLE:\n",
    "    print(\"ğŸ”„ Training Prophet Model...\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare data for Prophet (requires specific format)\n",
    "        prophet_train = train_data.reset_index()[['date', 'sales']].rename(\n",
    "            columns={'date': 'ds', 'sales': 'y'}\n",
    "        )\n",
    "        \n",
    "        # Create and fit Prophet model\n",
    "        prophet_model = Prophet(\n",
    "            daily_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            yearly_seasonality=True,\n",
    "            changepoint_prior_scale=0.05,  # Controls flexibility\n",
    "            seasonality_prior_scale=10.0   # Controls seasonality strength\n",
    "        )\n",
    "        \n",
    "        prophet_model.fit(prophet_train)\n",
    "        \n",
    "        # Create future dataframe for predictions\n",
    "        prophet_future = prophet_model.make_future_dataframe(\n",
    "            periods=len(test_data), freq='D'\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        prophet_forecast = prophet_model.predict(prophet_future)\n",
    "        \n",
    "        # Extract test predictions\n",
    "        prophet_test_pred = prophet_forecast.tail(len(test_data))['yhat'].values\n",
    "        \n",
    "        # Calculate metrics\n",
    "        prophet_metrics = calculate_metrics(test_data['sales'].values, prophet_test_pred, 'Prophet')\n",
    "        model_results['Prophet'] = {\n",
    "            'predictions': prophet_test_pred,\n",
    "            'forecast_df': prophet_forecast,\n",
    "            'metrics': prophet_metrics,\n",
    "            'model': prophet_model\n",
    "        }\n",
    "        \n",
    "        print(f\"   âœ… Prophet MAPE: {prophet_metrics['mape']:.2f}%\")\n",
    "        print(f\"   âœ… Prophet RÂ²: {prophet_metrics['r2']:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Prophet model failed: {e}\")\n",
    "        model_results['Prophet'] = None\nelse:\n    print(\"âš ï¸ Skipping Prophet model (not installed)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "source": [
    "# Model 3: Random Forest Regression\n",
    "print(\"ğŸ”„ Training Random Forest Model...\")\n",
    "\n",
    "try:\n",
    "    # Prepare features for ML model\n",
    "    feature_columns = ['year', 'month', 'day', 'weekday', 'is_weekend', \n",
    "                      'is_holiday_season', 'sales_lag1', 'sales_lag7', \n",
    "                      'sales_ma7', 'sales_ma30']\n",
    "    \n",
    "    # Prepare training data (remove NaN values)\n",
    "    X_train = train_data[feature_columns].dropna()\n",
    "    y_train = train_data.loc[X_train.index, 'sales']\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = test_data[feature_columns].dropna()\n",
    "    y_test = test_data.loc[X_test.index, 'sales']\n",
    "    \n",
    "    print(f\"   Training samples: {len(X_train)}\")\n",
    "    print(f\"   Test samples: {len(X_test)}\")\n",
    "    print(f\"   Features: {feature_columns}\")\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    rf_predictions = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rf_metrics = calculate_metrics(y_test.values, rf_predictions, 'Random Forest')\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    model_results['Random Forest'] = {\n",
    "        'predictions': rf_predictions,\n",
    "        'metrics': rf_metrics,\n",
    "        'model': rf_model,\n",
    "        'feature_importance': feature_importance,\n",
    "        'test_index': X_test.index\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… Random Forest MAPE: {rf_metrics['mape']:.2f}%\")\n",
    "    print(f\"   âœ… Random Forest RÂ²: {rf_metrics['r2']:.3f}\")\n",
    "    print(f\"   ğŸ” Top feature: {feature_importance.iloc[0]['feature']} ({feature_importance.iloc[0]['importance']:.3f})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Random Forest model failed: {e}\")\n",
    "    model_results['Random Forest'] = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "source": [
    "# Model 4: Linear Regression (Baseline)\n",
    "print(\"ğŸ”„ Training Linear Regression Model...\")\n",
    "\n",
    "try:\n",
    "    # Use same features as Random Forest\n",
    "    X_train = train_data[feature_columns].dropna()\n",
    "    y_train = train_data.loc[X_train.index, 'sales']\n",
    "    X_test = test_data[feature_columns].dropna()\n",
    "    y_test = test_data.loc[X_test.index, 'sales']\n",
    "    \n",
    "    # Scale features for linear regression\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train Linear Regression\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    lr_predictions = lr_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    lr_metrics = calculate_metrics(y_test.values, lr_predictions, 'Linear Regression')\n",
    "    \n",
    "    # Feature coefficients\n",
    "    feature_coefs = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'coefficient': lr_model.coef_\n",
    "    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    model_results['Linear Regression'] = {\n",
    "        'predictions': lr_predictions,\n",
    "        'metrics': lr_metrics,\n",
    "        'model': lr_model,\n",
    "        'scaler': scaler,\n",
    "        'feature_coefs': feature_coefs,\n",
    "        'test_index': X_test.index\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… Linear Regression MAPE: {lr_metrics['mape']:.2f}%\")\n",
    "    print(f\"   âœ… Linear Regression RÂ²: {lr_metrics['r2']:.3f}\")\n",
    "    print(f\"   ğŸ” Strongest coefficient: {feature_coefs.iloc[0]['feature']} ({feature_coefs.iloc[0]['coefficient']:.1f})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Linear Regression model failed: {e}\")\n",
    "    model_results['Linear Regression'] = None\n",
    "\n",
    "print(\"\\nğŸ‰ Model training completed!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## ğŸ“Š Model Comparison & Evaluation\n",
    "\n",
    "Let's compare all our models and identify the best performer for our sales forecasting task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "source": [
    "# Create model comparison table\n",
    "print(\"ğŸ“Š MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_data = []\n",
    "valid_models = {k: v for k, v in model_results.items() if v is not None}\n",
    "\n",
    "for model_name, results in valid_models.items():\n",
    "    metrics = results['metrics']\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'MAE': f\"${metrics['mae']:,.0f}\",\n",
    "        'RMSE': f\"${metrics['rmse']:,.0f}\",\n",
    "        'MAPE': f\"{metrics['mape']:.2f}%\",\n",
    "        'RÂ²': f\"{metrics['r2']:.3f}\",\n",
    "        'Direction Accuracy': f\"{metrics['direction_accuracy']:.1f}%\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model (lowest MAPE)\n",
    "best_model_name = min(valid_models.keys(), \n",
    "                     key=lambda x: valid_models[x]['metrics']['mape'])\n",
    "best_mape = valid_models[best_model_name]['metrics']['mape']\n",
    "\n",
    "print(f\"\\nğŸ† BEST MODEL: {best_model_name} (MAPE: {best_mape:.2f}%)\")\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('ğŸ† Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Extract metrics for plotting\n",
    "model_names = list(valid_models.keys())\n",
    "mape_scores = [valid_models[m]['metrics']['mape'] for m in model_names]\n",
    "r2_scores = [valid_models[m]['metrics']['r2'] for m in model_names]\n",
    "mae_scores = [valid_models[m]['metrics']['mae'] for m in model_names]\n",
    "direction_acc = [valid_models[m]['metrics']['direction_accuracy'] for m in model_names]\n",
    "\n",
    "# MAPE comparison (lower is better)\n",
    "bars1 = axes[0,0].bar(model_names, mape_scores, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('MAPE - Lower is Better')\n",
    "axes[0,0].set_ylabel('MAPE (%)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars1, mape_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                   f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# RÂ² comparison (higher is better)\n",
    "bars2 = axes[0,1].bar(model_names, r2_scores, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "axes[0,1].set_title('RÂ² Score - Higher is Better')\n",
    "axes[0,1].set_ylabel('RÂ² Score')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, value in zip(bars2, r2_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[0,1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# MAE comparison\n",
    "bars3 = axes[1,0].bar(model_names, mae_scores, color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[1,0].set_title('MAE - Lower is Better')\n",
    "axes[1,0].set_ylabel('MAE ($)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, value in zip(bars3, mae_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[1,0].text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "                   f'${value:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Direction Accuracy\n",
    "bars4 = axes[1,1].bar(model_names, direction_acc, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1,1].set_title('Direction Accuracy - Higher is Better')\n",
    "axes[1,1].set_ylabel('Direction Accuracy (%)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, value in zip(bars4, direction_acc):\n",
    "    height = bar.get_height()\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                   f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "source": [
    "# Visualize predictions vs actual values\n",
    "fig, axes = plt.subplots(len(valid_models), 1, figsize=(16, 4*len(valid_models)))\n",
    "if len(valid_models) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "fig.suptitle('ğŸ”® Model Predictions vs Actual Sales', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, (model_name, results) in enumerate(valid_models.items()):\n",
    "    predictions = results['predictions']\n",
    "    \n",
    "    # Get the appropriate test data indices\n",
    "    if 'test_index' in results:\n",
    "        test_dates = results['test_index']\n",
    "        actual_values = test_data.loc[test_dates, 'sales']\n",
    "    else:\n",
    "        test_dates = test_data.index\n",
    "        actual_values = test_data['sales']\n",
    "        # Ensure predictions and actual values have same length\n",
    "        min_len = min(len(predictions), len(actual_values))\n",
    "        predictions = predictions[:min_len]\n",
    "        actual_values = actual_values.iloc[:min_len]\n",
    "        test_dates = test_dates[:min_len]\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    axes[i].plot(test_dates, actual_values, label='Actual', color='navy', linewidth=2, alpha=0.8)\n",
    "    axes[i].plot(test_dates, predictions, label='Predicted', color='red', linewidth=2, alpha=0.8, linestyle='--')\n",
    "    \n",
    "    # Add confidence intervals for ARIMA if available\n",
    "    if model_name == 'ARIMA' and 'conf_int' in results:\n",
    "        conf_int = results['conf_int']\n",
    "        axes[i].fill_between(test_dates, \n",
    "                           conf_int.iloc[:, 0], \n",
    "                           conf_int.iloc[:, 1], \n",
    "                           alpha=0.2, color='red', label='95% Confidence')\n",
    "    \n",
    "    axes[i].set_title(f'{model_name} - MAPE: {results[\"metrics\"][\"mape\"]:.2f}%')\n",
    "    axes[i].set_ylabel('Sales ($)')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate business insights\n",
    "print(\"\\nğŸ’¡ BUSINESS INSIGHTS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if valid_models:\n",
    "    avg_actual = test_data['sales'].mean()\n",
    "    print(f\"ğŸ“Š Average actual daily sales: ${avg_actual:,.0f}\")\n",
    "    \n",
    "    for model_name, results in valid_models.items():\n",
    "        if 'test_index' in results:\n",
    "            avg_predicted = np.mean(results['predictions'])\n",
    "        else:\n",
    "            avg_predicted = np.mean(results['predictions'][:len(test_data)])\n",
    "        \n",
    "        bias = ((avg_predicted - avg_actual) / avg_actual) * 100\n",
    "        print(f\"   {model_name}: Avg predicted ${avg_predicted:,.0f} (bias: {bias:+.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Best model ({best_model_name}) insights:\")\n",
    "    best_results = valid_models[best_model_name]\n",
    "    print(f\"   ğŸ’° Forecast accuracy: {100 - best_results['metrics']['mape']:.1f}%\")\n",
    "    print(f\"   ğŸ“ˆ Direction accuracy: {best_results['metrics']['direction_accuracy']:.1f}%\")\n",
    "    print(f\"   ğŸ“Š Explains {best_results['metrics']['r2']:.1%} of sales variance\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## ğŸ”® Future Forecasting\n",
    "\n",
    "Now let's use our best model to generate actual forecasts for the next 30 days and create actionable business recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "source": [
    "# Generate future forecasts using the best model\n",
    "print(f\"ğŸ”® Generating future forecasts using {best_model_name}...\")\n",
    "\n",
    "forecast_days = 30\n",
    "last_date = df_ts.index.max()\n",
    "future_dates = pd.date_range(start=last_date + timedelta(days=1), \n",
    "                           periods=forecast_days, freq='D')\n",
    "\n",
    "best_results = valid_models[best_model_name]\n",
    "\n",
    "if best_model_name == 'ARIMA':\n",
    "    # ARIMA forecast\n",
    "    print(\"   Using ARIMA model for forecasting...\")\n",
    "    arima_model = best_results['model']\n",
    "    future_forecast = arima_model.forecast(steps=forecast_days)\n",
    "    future_conf_int = arima_model.get_forecast(steps=forecast_days).conf_int()\n",
    "    \n",
    "elif best_model_name == 'Prophet' and PROPHET_AVAILABLE:\n",
    "    # Prophet forecast\n",
    "    print(\"   Using Prophet model for forecasting...\")\n",
    "    prophet_model = best_results['model']\n",
    "    \n",
    "    # Create future dataframe\n",
    "    future_df = pd.DataFrame({'ds': future_dates})\n",
    "    future_prophet = prophet_model.predict(future_df)\n",
    "    future_forecast = future_prophet['yhat'].values\n",
    "    future_conf_int = future_prophet[['yhat_lower', 'yhat_upper']]\n",
    "    \n",
    "else:\n",
    "    # Machine Learning models need features\n",
    "    print(f\"   Using {best_model_name} model for forecasting...\")\n",
    "    \n",
    "    # Create future features\n",
    "    future_features = []\n",
    "    \n",
    "    for date in future_dates:\n",
    "        # Get recent sales data for lag features\n",
    "        recent_sales = df_ts['sales'].tail(30).values\n",
    "        \n",
    "        features = {\n",
    "            'year': date.year,\n",
    "            'month': date.month,\n",
    "            'day': date.day,\n",
    "            'weekday': date.weekday(),\n",
    "            'is_weekend': 1 if date.weekday() >= 5 else 0,\n",
    "            'is_holiday_season': 1 if date.month in [11, 12] else 0,\n",
    "            'sales_lag1': recent_sales[-1],\n",
    "            'sales_lag7': recent_sales[-7] if len(recent_sales) >= 7 else recent_sales[-1],\n",
    "            'sales_ma7': np.mean(recent_sales[-7:]) if len(recent_sales) >= 7 else recent_sales[-1],\n",
    "            'sales_ma30': np.mean(recent_sales) if len(recent_sales) >= 30 else np.mean(recent_sales)\n",
    "        }\n",
    "        future_features.append(features)\n",
    "    \n",
    "    future_df = pd.DataFrame(future_features)\n",
    "    \n",
    "    # Make predictions\n",
    "    if best_model_name == 'Linear Regression':\n",
    "        scaler = best_results['scaler']\n",
    "        future_scaled = scaler.transform(future_df[feature_columns])\n",
    "        future_forecast = best_results['model'].predict(future_scaled)\n",
    "    else:  # Random Forest\n",
    "        future_forecast = best_results['model'].predict(future_df[feature_columns])\n",
    "    \n",
    "    # Simple confidence intervals for ML models (Â±10%)\n",
    "    future_conf_int = pd.DataFrame({\n",
    "        'lower': future_forecast * 0.9,\n",
    "        'upper': future_forecast * 1.1\n",
    "    })\n",
    "\n",
    "# Create forecast results DataFrame\n",
    "forecast_results = pd.DataFrame({\n",
    "    'date': future_dates,\n",
    "    'forecast': future_forecast,\n",
    "    'lower_bound': future_conf_int.iloc[:, 0] if hasattr(future_conf_int, 'iloc') else future_conf_int['lower'],\n",
    "    'upper_bound': future_conf_int.iloc[:, 1] if hasattr(future_conf_int, 'iloc') else future_conf_int['upper']\n",
    "})\n",
    "\n",
    "print(f\"\\nğŸ“Š 30-Day Forecast Summary:\")\n",
    "print(f\"   ğŸ“… Forecast period: {future_dates[0].strftime('%Y-%m-%d')} to {future_dates[-1].strftime('%Y-%m-%d')}\")\n",
    "print(f\"   ğŸ’° Average daily forecast: ${future_forecast.mean():,.0f}\")\n",
    "print(f\"   ğŸ“Š Forecast range: ${future_forecast.min():,.0f} - ${future_forecast.max():,.0f}\")\n",
    "print(f\"   ğŸ’¼ Total forecasted revenue: ${future_forecast.sum():,.0f}\")\n",
    "\n",
    "# Compare to historical performance\n",
    "historical_avg = df_ts['sales'].tail(30).mean()\n",
    "forecast_vs_historical = ((future_forecast.mean() - historical_avg) / historical_avg) * 100\n",
    "\n",
    "print(f\"   ğŸ“ˆ vs Last 30 days: {forecast_vs_historical:+.1f}% change\")\n",
    "\n",
    "# Display forecast table\n",
    "print(f\"\\nğŸ“‹ Detailed Forecast (First 10 days):\")\n",
    "forecast_display = forecast_results.head(10).copy()\n",
    "forecast_display['forecast'] = forecast_display['forecast'].round(0).astype(int)\n",
    "forecast_display['lower_bound'] = forecast_display['lower_bound'].round(0).astype(int)\n",
    "forecast_display['upper_bound'] = forecast_display['upper_bound'].round(0).astype(int)\n",
    "forecast_display['date'] = forecast_display['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "print(forecast_display.to_string(index=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "source": [
    "# Visualize the forecast\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "fig.suptitle('ğŸ”® Sales Forecast Visualization', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Historical + Forecast plot\n",
    "historical_period = df_ts.tail(90)  # Last 90 days\n",
    "\n",
    "axes[0].plot(historical_period.index, historical_period['sales'], \n",
    "            label='Historical Sales', color='navy', linewidth=2, alpha=0.8)\n",
    "axes[0].plot(forecast_results['date'], forecast_results['forecast'], \n",
    "            label='Forecast', color='red', linewidth=2, alpha=0.8, linestyle='--')\n",
    "\n",
    "# Add confidence bands\n",
    "axes[0].fill_between(forecast_results['date'], \n",
    "                    forecast_results['lower_bound'], \n",
    "                    forecast_results['upper_bound'], \n",
    "                    alpha=0.2, color='red', label='Confidence Interval')\n",
    "\n",
    "# Add vertical line to separate historical from forecast\n",
    "axes[0].axvline(x=df_ts.index.max(), color='black', linestyle=':', alpha=0.7, \n",
    "               label='Forecast Start')\n",
    "\n",
    "axes[0].set_title('Historical Sales + 30-Day Forecast')\n",
    "axes[0].set_ylabel('Sales ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Forecast detail plot\n",
    "axes[1].plot(forecast_results['date'], forecast_results['forecast'], \n",
    "            marker='o', linewidth=2, markersize=4, color='red', alpha=0.8)\n",
    "axes[1].fill_between(forecast_results['date'], \n",
    "                    forecast_results['lower_bound'], \n",
    "                    forecast_results['upper_bound'], \n",
    "                    alpha=0.3, color='red')\n",
    "\n",
    "axes[1].set_title('30-Day Forecast Detail')\n",
    "axes[1].set_ylabel('Sales ($)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add weekend highlighting\n",
    "for date, forecast in zip(forecast_results['date'], forecast_results['forecast']):\n",
    "    if date.weekday() >= 5:  # Weekend\n",
    "        axes[1].plot(date, forecast, marker='s', markersize=6, color='orange', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Weekly forecast summary\n",
    "forecast_results['week'] = forecast_results['date'].dt.isocalendar().week\n",
    "weekly_forecast = forecast_results.groupby('week').agg({\n",
    "    'forecast': 'sum',\n",
    "    'date': ['min', 'max']\n",
    "}).round(0)\n",
    "\n",
    "weekly_forecast.columns = ['weekly_total', 'week_start', 'week_end']\n",
    "weekly_forecast['weekly_total'] = weekly_forecast['weekly_total'].astype(int)\n",
    "\n",
    "print(f\"\\nğŸ“Š Weekly Forecast Breakdown:\")\n",
    "for week, row in weekly_forecast.iterrows():\n",
    "    print(f\"   Week {week}: ${row['weekly_total']:,} ({row['week_start'].strftime('%m/%d')} - {row['week_end'].strftime('%m/%d')})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## ğŸ’¼ Business Recommendations\n",
    "\n",
    "Based on our comprehensive analysis and forecasting models, let me provide actionable business recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "source": [
    "# Generate comprehensive business recommendations\n",
    "print(\"ğŸ’¼ BUSINESS RECOMMENDATIONS & INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Forecasting Model Recommendations\n",
    "print(\"\\nğŸ¤– FORECASTING MODEL INSIGHTS:\")\n",
    "print(f\"   ğŸ† Recommended model: {best_model_name}\")\n",
    "print(f\"   ğŸ“Š Forecast accuracy: {100 - best_results['metrics']['mape']:.1f}%\")\n",
    "print(f\"   ğŸ¯ Model reliability: {best_results['metrics']['r2']:.1%} variance explained\")\n",
    "\n",
    "if best_results['metrics']['mape'] < 10:\n",
    "    print(\"   âœ… Excellent forecasting accuracy - suitable for operational planning\")\n",
    "elif best_results['metrics']['mape'] < 15:\n",
    "    print(\"   âœ… Good forecasting accuracy - suitable for strategic planning\")\n",
    "else:\n",
    "    print(\"   âš ï¸ Moderate accuracy - use with caution and frequent updates\")\n",
    "\n",
    "# 2. Inventory Management\n",
    "print(\"\\nğŸ“¦ INVENTORY MANAGEMENT:\")\n",
    "forecasted_total = future_forecast.sum()\n",
    "current_daily_avg = df_ts['sales'].tail(30).mean()\n",
    "\n",
    "print(f\"   ğŸ“Š Next 30-day demand: ${forecasted_total:,.0f}\")\n",
    "print(f\"   ğŸ“ˆ vs Current trend: {forecast_vs_historical:+.1f}% change\")\n",
    "\n",
    "if forecast_vs_historical > 10:\n",
    "    print(\"   ğŸ”¼ INCREASE inventory levels - higher demand expected\")\n",
    "    print(f\"   ğŸ’¡ Recommendation: Stock up {abs(forecast_vs_historical):.0f}% more than usual\")\n",
    "elif forecast_vs_historical < -10:\n",
    "    print(\"   ğŸ”½ REDUCE inventory levels - lower demand expected\")\n",
    "    print(f\"   ğŸ’¡ Recommendation: Reduce stock by {abs(forecast_vs_historical):.0f}%\")\n",
    "else:\n",
    "    print(\"   â¡ï¸ MAINTAIN current inventory levels - stable demand\")\n",
    "    print(\"   ğŸ’¡ Recommendation: Continue current procurement strategy\")\n",
    "\n",
    "# 3. Seasonal Insights\n",
    "print(\"\\nğŸ­ SEASONAL INSIGHTS:\")\n",
    "weekend_boost = (weekday_sales[5:].mean() - weekday_sales[:5].mean()) / weekday_sales[:5].mean() * 100\n",
    "print(f\"   ğŸ“… Weekend sales boost: +{weekend_boost:.1f}%\")\n",
    "print(f\"   ğŸ¯ Peak day: {weekday_names[weekday_sales.idxmax()]} (${weekday_sales.max():,.0f})\")\n",
    "print(f\"   ğŸ“‰ Low day: {weekday_names[weekday_sales.idxmin()]} (${weekday_sales.min():,.0f})\")\n",
    "\n",
    "# Holiday season analysis\n",
    "if any(date.month in [11, 12] for date in future_dates):\n",
    "    holiday_forecast = [f for f, date in zip(future_forecast, future_dates) if date.month in [11, 12]]\n",
    "    if holiday_forecast:\n",
    "        holiday_avg = np.mean(holiday_forecast)\n",
    "        regular_avg = np.mean([f for f, date in zip(future_forecast, future_dates) if date.month not in [11, 12]])\n",
    "        holiday_lift = ((holiday_avg - regular_avg) / regular_avg) * 100\n",
    "        print(f\"   ğŸ„ Holiday season lift: +{holiday_lift:.1f}% vs regular days\")\n",
    "\n",
    "# 4. Risk Assessment\n",
    "print(\"\\nâš ï¸ RISK ASSESSMENT:\")\n",
    "forecast_volatility = np.std(future_forecast) / np.mean(future_forecast) * 100\n",
    "historical_volatility = df_ts['sales'].std() / df_ts['sales'].mean() * 100\n",
    "\n",
    "print(f\"   ğŸ“Š Forecast volatility: {forecast_volatility:.1f}%\")\n",
    "print(f\"   ğŸ“Š Historical volatility: {historical_volatility:.1f}%\")\n",
    "\n",
    "if forecast_volatility > historical_volatility * 1.2:\n",
    "    print(\"   ğŸ”´ HIGH RISK: Increased volatility expected\")\n",
    "    print(\"   ğŸ’¡ Recommendation: Implement flexible inventory strategy\")\n",
    "elif forecast_volatility < historical_volatility * 0.8:\n",
    "    print(\"   ğŸŸ¢ LOW RISK: Stable period expected\")\n",
    "    print(\"   ğŸ’¡ Recommendation: Safe to optimize for efficiency\")\n",
    "else:\n",
    "    print(\"   ğŸŸ¡ MODERATE RISK: Normal volatility expected\")\n",
    "    print(\"   ğŸ’¡ Recommendation: Maintain current risk management practices\")\n",
    "\n",
    "# 5. Actionable Next Steps\n",
    "print(\"\\nğŸš€ ACTIONABLE NEXT STEPS:\")\n",
    "\n",
    "steps = [\n",
    "    f\"1. ğŸ“Š Implement {best_model_name} model for weekly sales forecasting\",\n",
    "    \"2. ğŸ“¦ Adjust inventory levels based on 30-day forecast\",\n",
    "    \"3. ğŸ‘¥ Schedule staff according to predicted busy/slow days\",\n",
    "    \"4. ğŸ“ˆ Monitor actual vs predicted sales for model accuracy\",\n",
    "    \"5. ğŸ”„ Update forecasts weekly with new sales data\"\n",
    "]\n",
    "\n",
    "for step in steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "# 6. Financial Impact\n",
    "print(\"\\nğŸ’° FINANCIAL IMPACT:\")\n",
    "monthly_forecast = forecasted_total\n",
    "current_monthly = current_daily_avg * 30\n",
    "revenue_impact = monthly_forecast - current_monthly\n",
    "\n",
    "print(f\"   ğŸ“ˆ Forecasted monthly revenue: ${monthly_forecast:,.0f}\")\n",
    "print(f\"   ğŸ“Š Current trend monthly: ${current_monthly:,.0f}\")\n",
    "print(f\"   ğŸ’µ Expected change: ${revenue_impact:+,.0f} ({revenue_impact/current_monthly*100:+.1f}%)\")\n",
    "\n",
    "if revenue_impact > 0:\n",
    "    print(f\"   âœ… Positive outlook - prepare for growth\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ Revenue decline expected - implement retention strategies\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“Š Analysis completed successfully!\")\n",
    "print(f\"ğŸ• Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"ğŸ¯ Ready for business decision-making!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Export Results\n",
    "\n",
    "Let's save our results for stakeholder sharing and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "source": [
    "# Export results to files\n",
    "print(\"ğŸ’¾ Exporting analysis results...\")\n",
    "\n",
    "try:\n",
    "    # 1. Export forecast results\n",
    "    forecast_export = forecast_results.copy()\n",
    "    forecast_export['model_used'] = best_model_name\n",
    "    forecast_export['forecast_accuracy'] = f\"{100 - best_results['metrics']['mape']:.1f}%\"\n",
    "    forecast_export.to_csv('sales_forecast_30_days.csv', index=False)\n",
    "    print(\"   âœ… sales_forecast_30_days.csv\")\n",
    "    \n",
    "    # 2. Export model comparison\n",
    "    comparison_export = pd.DataFrame([\n",
    "        {\n",
    "            'model': name,\n",
    "            'mape': results['metrics']['mape'],\n",
    "            'rmse': results['metrics']['rmse'],\n",
    "            'mae': results['metrics']['mae'],\n",
    "            'r2_score': results['metrics']['r2'],\n",
    "            'direction_accuracy': results['metrics']['direction_accuracy']\n",
    "        }\n",
    "        for name, results in valid_models.items()\n",
    "    ])\n",
    "    comparison_export.to_csv('model_performance_comparison.csv', index=False)\n",
    "    print(\"   âœ… model_performance_comparison.csv\")\n",
    "    \n",
    "    # 3. Export historical analysis\n",
    "    historical_analysis = df_ts[['sales', 'year', 'month', 'weekday', 'is_weekend']].copy()\n",
    "    historical_analysis['sales_ma7'] = historical_analysis['sales'].rolling(7).mean()\n",
    "    historical_analysis['sales_ma30'] = historical_analysis['sales'].rolling(30).mean()\n",
    "    historical_analysis.to_csv('historical_sales_analysis.csv')\n",
    "    print(\"   âœ… historical_sales_analysis.csv\")\n",
    "    \n",
    "    # 4. Export business insights summary\n",
    "    insights_summary = {\n",
    "        'analysis_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "        'best_model': best_model_name,\n",
    "        'model_accuracy': f\"{100 - best_results['metrics']['mape']:.1f}%\",\n",
    "        'forecast_period': f\"{future_dates[0].strftime('%Y-%m-%d')} to {future_dates[-1].strftime('%Y-%m-%d')}\",\n",
    "        'total_forecasted_revenue': f\"${future_forecast.sum():,.0f}\",\n",
    "        'avg_daily_forecast': f\"${future_forecast.mean():,.0f}\",\n",
    "        'vs_historical_trend': f\"{forecast_vs_historical:+.1f}%\",\n",
    "        'peak_sales_day': weekday_names[weekday_sales.idxmax()],\n",
    "        'weekend_boost': f\"{weekend_boost:+.1f}%\",\n",
    "        'forecast_volatility': f\"{forecast_volatility:.1f}%\"\n",
    "    }\n",
    "    \n",
    "    insights_df = pd.DataFrame([insights_summary])\n",
    "    insights_df.to_csv('business_insights_summary.csv', index=False)\n",
    "    print(\"   âœ… business_insights_summary.csv\")\n",
    "    \n",
    "    # 5. Export feature importance (if available)\n",
    "    if 'Random Forest' in valid_models and valid_models['Random Forest'] is not None:\n",
    "        rf_results = valid_models['Random Forest']\n",
    "        if 'feature_importance' in rf_results:\n",
    "            rf_results['feature_importance'].to_csv('feature_importance_analysis.csv', index=False)\n",
    "            print(\"   âœ… feature_importance_analysis.csv\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ All results exported successfully!\")\n",
    "    print(f\"ğŸ¯ Files ready for stakeholder review and business planning\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Export error: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Sales Forecasting Analysis Complete!\")\n",
    "print(f\"ğŸ”® 30-day forecast generated with {100 - best_results['metrics']['mape']:.1f}% accuracy\")\n",
    "print(f\"ğŸ’¼ Business recommendations ready for implementation\")\n",
    "print(f\"ğŸ“Š Thank you for following this comprehensive forecasting journey!\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}